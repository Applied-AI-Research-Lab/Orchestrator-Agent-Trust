{"cells":[{"cell_type":"markdown","metadata":{"id":"25gVGPluBfrm"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkKkeJB2Bfrm"},"outputs":[],"source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n","    !pip install transformers==4.51.3\n","    !pip install --no-deps unsloth\n","    !pip install optuna"]},{"cell_type":"markdown","metadata":{"id":"JJ-FlhZWBfrn"},"source":["### Unsloth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmUBVEnvCDJv"},"outputs":[],"source":["from unsloth import FastVisionModel # FastLanguageModel for LLMs\n","import torch\n","\n","# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n","fourbit_models = [\n","    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n","    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n","    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n","    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n","\n","    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n","    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n","\n","    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n","    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n","    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n","\n","    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n","    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n","] # More models at https://huggingface.co/unsloth\n","\n","model, tokenizer = FastVisionModel.from_pretrained(\n","    \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n","    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bZsfBuZDeCL"},"outputs":[],"source":["model = FastVisionModel.get_peft_model(\n","    model,\n","    finetune_vision_layers     = True, # False if not finetuning vision layers\n","    finetune_language_layers   = True, # False if not finetuning language layers\n","    finetune_attention_modules = True, # False if not finetuning attention layers\n","    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n","\n","    r = 16,           # The larger, the higher the accuracy, but might overfit\n","    lora_alpha = 16,  # Recommended alpha == r at least\n","    lora_dropout = 0,\n","    bias = \"none\",\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n","    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",")"]},{"cell_type":"markdown","source":["<a name=\"Convert\"></a>\n","### Convert train and validation CSVs to JSON format, structuring each conversation as a dictionary suitable for training LLMs."],"metadata":{"id":"TcCTz0l1MNSF"}},{"cell_type":"markdown","metadata":{"id":"K9CBpiISFa6C"},"source":["To format the data, all vision finetuning tasks should be formatted as follows:\n","\n","```python\n","[\n","{ \"role\": \"user\",\n","  \"content\": [{\"type\": \"text\",  \"text\": Q}, {\"type\": \"image\", \"image\": image} ]\n","},\n","{ \"role\": \"assistant\",\n","  \"content\": [{\"type\": \"text\",  \"text\": A} ]\n","},\n","]\n","```"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import pandas as pd\n","import json\n","\n","def generate_json_from_csv(\n","    csv_path: str,\n","    output_json_path: str,\n","    gcs_bucket_name: str,\n","    image_column: str = \"Image\",\n","    label_column: str = \"Category\"\n","):\n","    df = pd.read_csv(csv_path)\n","    data = []\n","\n","    for i, row in df.iterrows():\n","        img_path = str(row[image_column])\n","\n","        # Construct GCS URL if not already a full URL\n","        if not img_path.startswith(\"http\"):\n","            full_img_url = f\"https://storage.googleapis.com/{gcs_bucket_name}/{str(row[label_column])}/{img_path}\"\n","        else:\n","            full_img_url = img_path\n","\n","        prompt = (\n","            \"Analyze the provided image of an apple leaf using your computer vision capabilities. \"\n","            \"Classify the leaf into the most appropriate category based on its condition, \"\n","            \"choosing from the predefined list: \"\n","            \"{\\n  \\\"categories\\\": [\\n    \\\"black-rot\\\",\\n    \\\"healthy\\\",\\n    \\\"rust\\\",\\n    \\\"scab\\\"\\n  ]\\n} \"\n","            \"Provide your final classification in the following JSON format without explanations: \"\n","            \"{\\\"category\\\": \\\"chosen_category_name\\\"}\"\n","        )\n","\n","        conversation = { # prompt-completion pairs\n","            \"messages\": [\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": [\n","                        {\"type\": \"text\", \"text\": prompt},\n","                        {\"type\": \"image\", \"image\": full_img_url}\n","                    ]\n","                },\n","                {\n","                    \"role\": \"assistant\",\n","                    \"content\": [\n","                        {\"type\": \"text\", \"text\": f\"{{\\\"category\\\": \\\"{row[label_column]}\\\"}}\"}\n","                    ]\n","                }\n","            ]\n","        }\n","\n","        data.append(conversation)\n","\n","    # Save to JSON\n","    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(data, f, ensure_ascii=False, indent=2)\n","\n","    print(f\"CSV data saved to {output_json_path}\")\n","\n","def prepare_test_set(input_csv, gcs_bucket_name, output_csv):\n","    df = pd.read_csv(input_csv)\n","\n","    # Update the Image column\n","    df['Image'] = df.apply(\n","        lambda row: f\"https://storage.googleapis.com/{gcs_bucket_name}/{row['Category']}/{row['Image']}\", axis=1\n","    )\n","\n","    # Save the updated DataFrame to output_csv\n","    df.to_csv(output_csv, index=False)\n","\n","    print(f\"Updated CSV saved as {output_csv}\")\n","\n","absolute_path = \"/content/gdrive/My Drive/Projects/VL-Models/Datasets/\"\n","gcs_bucket_name = \"kroumeliotis-image-bucket/plant-disease-detection/Apple/256\"\n","\n","# generate_json_from_csv(\n","#     csv_path = absolute_path + \"train_set.csv\",\n","#     output_json_path = absolute_path + \"train_set_256.json\",\n","#     gcs_bucket_name = gcs_bucket_name\n","# )\n","\n","# generate_json_from_csv(\n","#     csv_path = absolute_path + \"validation_set.csv\",\n","#     output_json_path = absolute_path + \"validation_set_256.json\",\n","#     gcs_bucket_name = gcs_bucket_name\n","# )\n","\n","# prepare_test_set(\n","#     input_csv = absolute_path + \"test_set.csv\",\n","#     gcs_bucket_name = gcs_bucket_name,\n","#     output_csv = absolute_path + \"test_set_256.csv\"\n","# )"],"metadata":{"id":"U1Tjyu0VJv9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load train and validation sets into memory"],"metadata":{"id":"WZK7PssPTcgf"}},{"cell_type":"code","source":["import json\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive') # We can omit it on sequence run\n","absolute_path = '/content/gdrive/My Drive/Projects/VL-Models/Datasets/'  # We can omit it on sequence run\n","\n","train_set = absolute_path + \"train_set_256.json\"\n","validation_set = absolute_path + \"validation_set_256.json\"\n","\n","# Load the dataset into memory\n","with open(train_set, \"r\", encoding=\"utf-8\") as f:\n","    train_set = json.load(f)\n","\n","with open(validation_set, \"r\", encoding=\"utf-8\") as f:\n","    validation_set = json.load(f)\n","\n","print(train_set[0])\n","print(validation_set[0])"],"metadata":{"id":"ptaF5sOGTiyR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FecKS-dA82f5"},"source":["### Zero-shot Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcat4UxA81vr"},"outputs":[],"source":["from PIL import Image\n","import requests\n","from io import BytesIO\n","\n","FastVisionModel.for_inference(model) # Enable for inference!\n","\n","sample = train_set[2]\n","\n","# Extract user message contents\n","user_content = sample[\"messages\"][0][\"content\"]\n","\n","# Extract image URL and instruction from the nested structure\n","instruction = next(item[\"text\"] for item in user_content if item[\"type\"] == \"text\")\n","image_url = next(item[\"image\"] for item in user_content if item[\"type\"] == \"image\")\n","# Download and load the image from URL\n","response = requests.get(image_url)\n","image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": [\n","        {\"type\": \"image\"},\n","        {\"type\": \"text\", \"text\": instruction}\n","    ]}\n","]\n","\n","input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n","inputs = tokenizer(\n","    image,\n","    input_text,\n","    add_special_tokens = False,\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["## Fine-Tuning Phase + Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL"},"outputs":[],"source":["!pip install optuna\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","drive_path = \"/content/gdrive/My Drive/Projects/VL-Models/Results/Qwen2.5-VL-7B-Instruct-bnb-4bit-4/\"\n","\n","import optuna\n","import time\n","import logging\n","import os\n","import pickle\n","import json\n","from typing import Dict, Any, Optional\n","from transformers import TrainerCallback, TrainerState, TrainerControl, EarlyStoppingCallback\n","import numpy as np\n","from unsloth import is_bf16_supported\n","from unsloth.trainer import UnslothVisionDataCollator\n","from trl import SFTTrainer, SFTConfig\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Persistence paths\n","STUDY_DB_PATH = f\"{drive_path}optuna_study.db\"\n","RESULTS_PATH = f\"{drive_path}trial_results.json\"\n","CHECKPOINT_PATH = f\"{drive_path}optimization_checkpoint.pkl\"\n","\n","class PersistentMetricsTracker:\n","    \"\"\"Track and store metrics for each trial with persistence\"\"\"\n","\n","    def __init__(self, results_path: str):\n","        self.results_path = results_path\n","        self.trial_results = self.load_existing_results()\n","\n","    def load_existing_results(self) -> list:\n","        \"\"\"Load existing trial results from file\"\"\"\n","        if os.path.exists(self.results_path):\n","            try:\n","                with open(self.results_path, 'r') as f:\n","                    results = json.load(f)\n","                print(f\"Loaded {len(results)} existing trial results\")\n","                return results\n","            except Exception as e:\n","                print(f\"Error loading existing results: {e}\")\n","                return []\n","        return []\n","\n","    def save_results(self):\n","        \"\"\"Save current trial results to file\"\"\"\n","        try:\n","            with open(self.results_path, 'w') as f:\n","                json.dump(self.trial_results, f, indent=2)\n","        except Exception as e:\n","            print(f\"Error saving results: {e}\")\n","\n","    def log_trial_result(self, trial_number: int, params: Dict[str, Any],\n","                        final_loss: float, training_time: float,\n","                        training_losses: list, validation_losses: list,\n","                        trial_state: str = \"COMPLETE\"):\n","        result = {\n","            'trial_number': trial_number,\n","            'params': params,\n","            'final_validation_loss': final_loss,\n","            'training_time_minutes': training_time / 60,\n","            'training_losses': training_losses,\n","            'validation_losses': validation_losses,\n","            'trial_state': trial_state,\n","            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n","        }\n","\n","        # Check if trial already exists and update, otherwise append\n","        existing_trial_idx = None\n","        for i, existing_result in enumerate(self.trial_results):\n","            if existing_result['trial_number'] == trial_number:\n","                existing_trial_idx = i\n","                break\n","\n","        if existing_trial_idx is not None:\n","            self.trial_results[existing_trial_idx] = result\n","        else:\n","            self.trial_results.append(result)\n","\n","        # Save immediately after each trial\n","        self.save_results()\n","\n","        # Print trial results\n","        print(f\"\\n{'='*60}\")\n","        print(f\"TRIAL {trial_number} {trial_state}\")\n","        print(f\"{'='*60}\")\n","        print(f\"Parameters:\")\n","        for key, value in params.items():\n","            print(f\"  {key}: {value}\")\n","        if final_loss != float('inf'):\n","            print(f\"Final Validation Loss: {final_loss:.6f}\")\n","        print(f\"Training Time: {training_time/60:.2f} minutes\")\n","        print(f\"{'='*60}\\n\")\n","\n","    def get_completed_trial_numbers(self) -> set:\n","        \"\"\"Get set of completed trial numbers\"\"\"\n","        completed = set()\n","        for result in self.trial_results:\n","            if result['trial_state'] == 'COMPLETE':\n","                completed.add(result['trial_number'])\n","        return completed\n","\n","class OptunaPruningCallback(TrainerCallback):\n","    \"\"\"Callback to enable Optuna pruning during training\"\"\"\n","\n","    def __init__(self, trial, monitor_metric=\"eval_loss\"):\n","        self.trial = trial\n","        self.monitor_metric = monitor_metric\n","\n","    def on_evaluate(self, args, state, control, model=None, **kwargs):\n","        # Report intermediate value to Optuna for pruning\n","        if state.log_history:\n","            # Get the latest evaluation metrics\n","            latest_logs = state.log_history[-1]\n","            if self.monitor_metric in latest_logs:\n","                current_value = latest_logs[self.monitor_metric]\n","                self.trial.report(current_value, state.epoch)\n","\n","                # Check if trial should be pruned\n","                if self.trial.should_prune():\n","                    raise optuna.TrialPruned(f\"Trial pruned at epoch {state.epoch}\")\n","\n","def save_checkpoint(study, metrics_tracker, current_trial: int):\n","    \"\"\"Save optimization checkpoint\"\"\"\n","    checkpoint_data = {\n","        'current_trial': current_trial,\n","        'study_trials_count': len(study.trials),\n","        'completed_trials': metrics_tracker.get_completed_trial_numbers(),\n","        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n","    }\n","\n","    try:\n","        with open(CHECKPOINT_PATH, 'wb') as f:\n","            pickle.dump(checkpoint_data, f)\n","        print(f\"Checkpoint saved at trial {current_trial}\")\n","    except Exception as e:\n","        print(f\"Error saving checkpoint: {e}\")\n","\n","def load_checkpoint() -> Optional[Dict]:\n","    \"\"\"Load optimization checkpoint\"\"\"\n","    if os.path.exists(CHECKPOINT_PATH):\n","        try:\n","            with open(CHECKPOINT_PATH, 'rb') as f:\n","                checkpoint = pickle.load(f)\n","            print(f\"Loaded checkpoint from {checkpoint['timestamp']}\")\n","            return checkpoint\n","        except Exception as e:\n","            print(f\"Error loading checkpoint: {e}\")\n","    return None\n","\n","def create_or_load_study():\n","    \"\"\"Create new study or load existing one\"\"\"\n","    storage_url = f\"sqlite:///{STUDY_DB_PATH}\"\n","    study_name = \"llama_hyperparameter_optimization\"\n","\n","    try:\n","        # Try to load existing study\n","        study = optuna.load_study(\n","            study_name=study_name,\n","            storage=storage_url\n","        )\n","        print(f\"Loaded existing study with {len(study.trials)} trials\")\n","        return study\n","    except KeyError:\n","        # Create new study if it doesn't exist\n","        print(\"Creating new study...\")\n","        study = optuna.create_study(\n","            study_name=study_name,\n","            storage=storage_url,\n","            direction=\"minimize\",\n","            pruner=optuna.pruners.MedianPruner(\n","                n_startup_trials=5,\n","                n_warmup_steps=5,\n","                interval_steps=2\n","            ),\n","            sampler=optuna.samplers.TPESampler(seed=3407),\n","            load_if_exists=True\n","        )\n","        return study\n","\n","# Initialize persistent metrics tracker\n","metrics_tracker = PersistentMetricsTracker(RESULTS_PATH)\n","\n","def objective(trial):\n","    \"\"\"Objective function for Optuna optimization\"\"\"\n","\n","    # Check if this trial was already completed\n","    completed_trials = metrics_tracker.get_completed_trial_numbers()\n","    if trial.number in completed_trials:\n","        print(f\"Trial {trial.number} was already completed, skipping...\")\n","        # Find the existing result and return its loss\n","        for result in metrics_tracker.trial_results:\n","            if (result['trial_number'] == trial.number and\n","                result['trial_state'] == 'COMPLETE'):\n","                return result['final_validation_loss']\n","        return float('inf')\n","\n","    # Suggest hyperparameters\n","    # learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 5e-4, log=True)\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True)\n","    # per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [1, 2, 4])\n","    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4])\n","    gradient_accumulation_steps = trial.suggest_categorical(\"gradient_accumulation_steps\", [4, 8, 16])\n","    # warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.3)\n","    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.01, 0.1)\n","    # weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n","    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.05)\n","    # num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 6)\n","    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 6, 15)\n","\n","    # Calculate warmup steps based on dataset size and batch configuration\n","    total_samples = len(train_set)\n","    steps_per_epoch = total_samples // (per_device_train_batch_size * gradient_accumulation_steps)\n","    total_steps = steps_per_epoch * num_train_epochs\n","    warmup_steps = int(total_steps * warmup_ratio)\n","\n","    print(f\"\\nStarting Trial {trial.number}\")\n","    print(f\"Parameters: {trial.params}\")\n","\n","    try:\n","        # Record start time\n","        start_time = time.time()\n","\n","        # Enable model for training\n","        FastVisionModel.for_training(model)\n","\n","        # Create trial-specific output directory\n","        trial_output_dir = f\"{drive_path}outputs/trial_{trial.number}\"\n","        os.makedirs(trial_output_dir, exist_ok=True)\n","\n","        # Create trainer with suggested hyperparameters\n","        trainer = SFTTrainer(\n","            model=model,\n","            tokenizer=tokenizer,\n","            data_collator=UnslothVisionDataCollator(model, tokenizer),\n","            train_dataset=train_set,\n","            eval_dataset=validation_set,\n","            args=SFTConfig(\n","                per_device_train_batch_size=per_device_train_batch_size,\n","                gradient_accumulation_steps=gradient_accumulation_steps,\n","                warmup_steps=warmup_steps,\n","                num_train_epochs=num_train_epochs,\n","                learning_rate=learning_rate,\n","                fp16=not is_bf16_supported(),\n","                bf16=is_bf16_supported(),\n","                logging_steps=1,\n","                optim=\"adamw_8bit\",\n","                weight_decay=weight_decay,\n","                lr_scheduler_type=\"linear\",\n","                seed=3407,\n","                output_dir=trial_output_dir,\n","                report_to=\"none\",\n","\n","                # Evaluation settings for pruning\n","                eval_strategy=\"epoch\",\n","                eval_steps=1,\n","                save_strategy= \"epoch\", #\"no\", do not save weights on each checkpoint\n","                save_total_limit=1,\n","                load_best_model_at_end=True,\n","                metric_for_best_model=\"eval_loss\",\n","                greater_is_better=False,\n","\n","                # Vision-specific settings\n","                remove_unused_columns=False,\n","                dataset_text_field=\"messages\",\n","                dataset_kwargs={\"skip_prepare_dataset\": True},\n","                dataset_num_proc=4,\n","                max_seq_length=2048,\n","            ),\n","        )\n","\n","        # Add callbacks for pruning and early stopping\n","        pruning_callback = OptunaPruningCallback(trial, monitor_metric=\"eval_loss\")\n","        early_stopping_callback = EarlyStoppingCallback(\n","            early_stopping_patience=5,\n","            early_stopping_threshold=0.003\n","        )\n","        trainer.add_callback(pruning_callback)\n","        trainer.add_callback(early_stopping_callback)\n","\n","        # Train the model\n","        trainer_stats = trainer.train()\n","\n","        # Record end time\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # Extract metrics from training history\n","        training_losses = []\n","        validation_losses = []\n","\n","        for log_entry in trainer.state.log_history:\n","            if 'train_loss' in log_entry:\n","                training_losses.append(log_entry['train_loss'])\n","            if 'eval_loss' in log_entry:\n","                validation_losses.append(log_entry['eval_loss'])\n","\n","        # Get final validation loss\n","        final_validation_loss = validation_losses[-1] if validation_losses else float('inf')\n","\n","        # Log trial results with COMPLETE state\n","        metrics_tracker.log_trial_result(\n","            trial_number=trial.number,\n","            params=trial.params,\n","            final_loss=final_validation_loss,\n","            training_time=training_time,\n","            training_losses=training_losses,\n","            validation_losses=validation_losses,\n","            trial_state=\"COMPLETE\"\n","        )\n","\n","        return final_validation_loss\n","\n","    except optuna.TrialPruned as e:\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        print(f\"Trial {trial.number} was pruned early: {str(e)}\")\n","\n","        # Log pruned trial results\n","        metrics_tracker.log_trial_result(\n","            trial_number=trial.number,\n","            params=trial.params,\n","            final_loss=float('inf'),\n","            training_time=training_time,\n","            training_losses=[],\n","            validation_losses=[],\n","            trial_state=\"PRUNED\"\n","        )\n","        raise\n","\n","    except Exception as e:\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        print(f\"Trial {trial.number} failed with error: {str(e)}\")\n","\n","        # Log failed trial results\n","        metrics_tracker.log_trial_result(\n","            trial_number=trial.number,\n","            params=trial.params,\n","            final_loss=float('inf'),\n","            training_time=training_time,\n","            training_losses=[],\n","            validation_losses=[],\n","            trial_state=\"FAILED\"\n","        )\n","        return float('inf')\n","\n","def run_optimization_with_resume(n_trials: int = 20):\n","    \"\"\"Run optimization with resume capability\"\"\"\n","\n","    # Create or load study\n","    study = create_or_load_study()\n","\n","    # Load checkpoint if available\n","    checkpoint = load_checkpoint()\n","    start_trial = 0\n","\n","    if checkpoint:\n","        completed_trials = metrics_tracker.get_completed_trial_numbers()\n","        start_trial = len(completed_trials)\n","        print(f\"Resuming from trial {start_trial} (completed: {len(completed_trials)})\")\n","\n","    # Calculate remaining trials\n","    remaining_trials = max(0, n_trials - len(study.trials))\n","\n","    if remaining_trials == 0:\n","        print(f\"All {n_trials} trials already completed!\")\n","        return study\n","\n","    print(f\"Running {remaining_trials} remaining trials...\")\n","    print(\"Each trial will be saved immediately and can be resumed if interrupted.\\n\")\n","\n","    # Custom optimization loop with checkpointing\n","    for i in range(remaining_trials):\n","        try:\n","            # Save checkpoint before each trial\n","            save_checkpoint(study, metrics_tracker, len(study.trials))\n","\n","            # Only enqueue a starting point if it hasn't already been tried\n","            if len(study.trials) == 0:\n","                study.enqueue_trial({\n","                    \"learning_rate\": 2e-4,\n","                    \"per_device_train_batch_size\": 2,\n","                    \"gradient_accumulation_steps\": 8,\n","                    \"warmup_ratio\": 0.05,  # estimated from warmup_steps=5\n","                    \"weight_decay\": 0.01,\n","                    \"num_train_epochs\": 10\n","                })\n","\n","            # Run single trial\n","            study.optimize(objective, n_trials=1, timeout=None)\n","\n","            print(f\"Progress: {len(study.trials)}/{n_trials} trials completed\")\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nOptimization interrupted by user\")\n","            break\n","        except Exception as e:\n","            print(f\"Error in trial: {e}\")\n","            print(\"Continuing with next trial...\")\n","            continue\n","\n","    return study\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Configuration\n","    n_trials = 50  # Adjust based on your needs\n","\n","    print(f\"Starting/Resuming Bayesian Optimization with up to {n_trials} trials...\")\n","    print(f\"Results will be saved to: {RESULTS_PATH}\")\n","    print(f\"Study database: {STUDY_DB_PATH}\")\n","    print(f\"Checkpoints: {CHECKPOINT_PATH}\\n\")\n","\n","    # Run optimization\n","    study = run_optimization_with_resume(n_trials)\n","\n","    # Print final results\n","    print(f\"\\n{'='*80}\")\n","    print(\"OPTIMIZATION STATUS\")\n","    print(f\"{'='*80}\")\n","\n","    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n","    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n","    failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]\n","\n","    print(f\"Total trials: {len(study.trials)}\")\n","    print(f\"Completed trials: {len(completed_trials)}\")\n","    print(f\"Pruned trials: {len(pruned_trials)}\")\n","    print(f\"Failed trials: {len(failed_trials)}\")\n","\n","    if completed_trials:\n","        print(f\"\\nBest trial:\")\n","        best_trial = study.best_trial\n","        print(f\"  Value (Validation Loss): {best_trial.value:.6f}\")\n","        print(f\"  Trial Number: {best_trial.number}\")\n","        print(f\"  Params:\")\n","        for key, value in best_trial.params.items():\n","            print(f\"    {key}: {value}\")\n","\n","        # Print top 5 trials\n","        print(f\"\\nTop 5 trials:\")\n","        top_trials = sorted(completed_trials, key=lambda x: x.value)[:5]\n","\n","        for i, trial in enumerate(top_trials, 1):\n","            print(f\"\\n  Rank {i}:\")\n","            print(f\"    Validation Loss: {trial.value:.6f}\")\n","            print(f\"    Trial Number: {trial.number}\")\n","            print(f\"    Params: {trial.params}\")\n","\n","        # Option to train final model with best hyperparameters\n","        retrain_best_model = True  # Set to False if you don't want to retrain\n","\n","        if retrain_best_model and completed_trials:\n","            print(f\"\\n{'='*60}\")\n","            print(\"TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n","            print(f\"{'='*60}\")\n","\n","            best_params = best_trial.params\n","\n","            # Calculate warmup steps for best params\n","            total_samples = len(train_set)\n","            steps_per_epoch = total_samples // (best_params['per_device_train_batch_size'] *\n","                                               best_params['gradient_accumulation_steps'])\n","            total_steps = steps_per_epoch * best_params['num_train_epochs']\n","            warmup_steps = int(total_steps * best_params['warmup_ratio'])\n","\n","            # Create final trainer\n","            FastVisionModel.for_training(model)\n","\n","            final_output_dir = f\"{drive_path}final_model\"\n","            os.makedirs(final_output_dir, exist_ok=True)\n","\n","            final_trainer = SFTTrainer(\n","                model=model,\n","                tokenizer=tokenizer,\n","                data_collator=UnslothVisionDataCollator(model, tokenizer),\n","                train_dataset=train_set,\n","                eval_dataset=validation_set,\n","                args=SFTConfig(\n","                    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n","                    gradient_accumulation_steps=best_params['gradient_accumulation_steps'],\n","                    warmup_steps=warmup_steps,\n","                    num_train_epochs=best_params['num_train_epochs'],\n","                    learning_rate=best_params['learning_rate'],\n","                    fp16=not is_bf16_supported(),\n","                    bf16=is_bf16_supported(),\n","                    logging_steps=1,\n","                    optim=\"adamw_8bit\",\n","                    weight_decay=best_params['weight_decay'],\n","                    lr_scheduler_type=\"linear\",\n","                    seed=3407,\n","                    output_dir=final_output_dir,\n","                    report_to=\"none\",\n","\n","                    eval_strategy=\"epoch\",\n","                    save_strategy=\"epoch\",\n","                    save_total_limit=3,\n","                    load_best_model_at_end=True,\n","                    metric_for_best_model=\"eval_loss\",\n","                    greater_is_better=False,\n","\n","                    remove_unused_columns=False,\n","                    dataset_text_field=\"messages\",\n","                    dataset_kwargs={\"skip_prepare_dataset\": True},\n","                    dataset_num_proc=4,\n","                    max_seq_length=2048,\n","                ),\n","            )\n","\n","            # Add early stopping callback to final trainer\n","            final_early_stopping = EarlyStoppingCallback(\n","                early_stopping_patience=3,\n","                early_stopping_threshold=0.01\n","            )\n","            final_trainer.add_callback(final_early_stopping)\n","\n","            # Train final model\n","            print(\"Training final model...\")\n","            final_stats = final_trainer.train()\n","\n","            # Save the best model\n","            best_model_path = f\"{drive_path}best_model\"\n","            os.makedirs(best_model_path, exist_ok=True)\n","            final_trainer.save_model(best_model_path)\n","            tokenizer.save_pretrained(best_model_path)\n","\n","            print(f\"Final model saved to '{best_model_path}'!\")\n","\n","    print(f\"\\n{'='*60}\")\n","    print(\"HYPERPARAMETER OPTIMIZATION COMPLETE!\")\n","    print(f\"{'='*60}\")\n","\n","    # Save final comprehensive results\n","    final_results = {\n","        'study_trials': len(study.trials),\n","        'completed_trials': len(completed_trials),\n","        'best_params': study.best_trial.params if completed_trials else None,\n","        'best_loss': study.best_trial.value if completed_trials else None,\n","        'all_trial_results': metrics_tracker.trial_results\n","    }\n","\n","    final_results_path = f'{drive_path}final_optimization_results.json'\n","    with open(final_results_path, 'w') as f:\n","        json.dump(final_results, f, indent=2)\n","\n","    print(f\"Complete results saved to '{final_results_path}'\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"https://github.com/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb","timestamp":1748102518715}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}